# generated by fastapi-codegen:
#   filename:  openapi.yaml
#   timestamp: 2025-06-29T02:55:43+00:00

from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field


class LongRunningRecognizeMetadata(BaseModel):
    lastUpdateTime: Optional[str] = Field(
        None, description='Output only. Time of the most recent processing update.'
    )
    progressPercent: Optional[int] = Field(
        None,
        description='Output only. Approximate percentage of audio processed thus far. Guaranteed to be 100 when the audio is fully processed and the results are available.',
    )
    startTime: Optional[str] = Field(
        None, description='Output only. Time when the request was received.'
    )
    uri: Optional[str] = Field(
        None,
        description='The URI of the audio file being transcribed. Empty if the audio was sent as byte content.',
    )


class Status(BaseModel):
    code: Optional[int] = Field(
        None,
        description='The status code, which should be an enum value of google.rpc.Code.',
    )
    details: Optional[List[Dict[str, Any]]] = Field(
        None,
        description='A list of messages that carry the error details. There is a common set of message types for APIs to use.',
    )
    message: Optional[str] = Field(
        None,
        description='A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.',
    )


class WordInfo(BaseModel):
    confidence: Optional[float] = Field(
        None,
        description='Output only. The confidence estimate between 0.0 and 1.0. A higher number indicates an estimated greater likelihood that the recognized words are correct. This field is set only for the top alternative of a non-streaming result or, of a streaming result where `is_final=true`. This field is not guaranteed to be accurate and users should not rely on it to be always provided. The default of 0.0 is a sentinel value indicating `confidence` was not set.',
    )
    endOffset: Optional[str] = Field(
        None,
        description='Output only. Time offset relative to the beginning of the audio, and corresponding to the end of the spoken word. This field is only set if `enable_word_time_offsets=true` and only in the top hypothesis. This is an experimental feature and the accuracy of the time offset can vary.',
    )
    speakerTag: Optional[int] = Field(
        None,
        description='Output only. A distinct integer value is assigned for every speaker within the audio. This field specifies which one of those speakers was detected to have spoken this word. Value ranges from `1` to `diarization_config.max_speaker_count` . `speaker_tag` is set if `diarization_config.enable_speaker_diarization` = `true` and only in the top alternative.',
    )
    startOffset: Optional[str] = Field(
        None,
        description='Output only. Time offset relative to the beginning of the audio, and corresponding to the start of the spoken word. This field is only set if `enable_word_time_offsets=true` and only in the top hypothesis. This is an experimental feature and the accuracy of the time offset can vary.',
    )
    word: Optional[str] = Field(
        None,
        description='Output only. The word corresponding to this set of information.',
    )


class FieldXgafv(Enum):
    field_1 = '1'
    field_2 = '2'


class Alt(Enum):
    json = 'json'
    media = 'media'
    proto = 'proto'


class Operation(BaseModel):
    done: Optional[bool] = Field(
        None,
        description='If the value is `false`, it means the operation is still in progress. If `true`, the operation is completed, and either `error` or `response` is available.',
    )
    error: Optional[Status] = Field(
        None,
        description='The error result of the operation in case of failure or cancellation.',
    )
    metadata: Optional[Dict[str, Any]] = Field(
        None,
        description='Service-specific metadata associated with the operation. It typically contains progress information and common metadata such as create time. Some services might not provide such metadata. Any method that returns a long-running operation should document the metadata type, if any.',
    )
    name: Optional[str] = Field(
        None,
        description='The server-assigned name, which is only unique within the same service that originally returns it. If you use the default HTTP mapping, the `name` should be a resource name ending with `operations/{unique_id}`.',
    )
    response: Optional[Dict[str, Any]] = Field(
        None,
        description='The normal response of the operation in case of success. If the original method returns no data on success, such as `Delete`, the response is `google.protobuf.Empty`. If the original method is standard `Get`/`Create`/`Update`, the response should be the resource. For other methods, the response should have the type `XxxResponse`, where `Xxx` is the original method name. For example, if the original method name is `TakeSnapshot()`, the inferred response type is `TakeSnapshotResponse`.',
    )


class SpeechRecognitionAlternative(BaseModel):
    confidence: Optional[float] = Field(
        None,
        description='Output only. The confidence estimate between 0.0 and 1.0. A higher number indicates an estimated greater likelihood that the recognized words are correct. This field is set only for the top alternative of a non-streaming result or, of a streaming result where `is_final=true`. This field is not guaranteed to be accurate and users should not rely on it to be always provided. The default of 0.0 is a sentinel value indicating `confidence` was not set.',
    )
    transcript: Optional[str] = Field(
        None,
        description='Output only. Transcript text representing the words that the user spoke.',
    )
    words: Optional[List[WordInfo]] = Field(
        None,
        description='Output only. A list of word-specific information for each recognized word. Note: When `enable_speaker_diarization` is true, you will see all the words from the beginning of the audio.',
    )


class SpeechRecognitionResult(BaseModel):
    alternatives: Optional[List[SpeechRecognitionAlternative]] = Field(
        None,
        description='Output only. May contain one or more recognition hypotheses (up to the maximum specified in `max_alternatives`). These alternatives are ordered in terms of accuracy, with the top (first) alternative being the most probable, as ranked by the recognizer.',
    )
    channelTag: Optional[int] = Field(
        None,
        description='Output only. For multi-channel audio, this is the channel number corresponding to the recognized result for the audio from that channel. For `audio_channel_count` = N, its output values can range from `1` to `N`.',
    )
    languageCode: Optional[str] = Field(
        None,
        description='Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the language in this result. This language code was detected to have the most likelihood of being spoken in the audio.',
    )


class ListOperationsResponse(BaseModel):
    nextPageToken: Optional[str] = Field(
        None, description='The standard List next-page token.'
    )
    operations: Optional[List[Operation]] = Field(
        None,
        description='A list of operations that matches the specified filter in the request.',
    )


class LongRunningRecognizeResponse(BaseModel):
    results: Optional[List[SpeechRecognitionResult]] = Field(
        None,
        description='Output only. Sequential list of transcription results corresponding to sequential portions of audio.',
    )
